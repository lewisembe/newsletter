#Idea

dame contenido de claude.md dando las siguientes instrucciones. El objetivo del proyecto es generar un pipeline de scripts que usando selenium open ai api y otras librerias permiten automatizar un flujo en el que la entrada sera una lista de topics que existen en las noticias de prensa (tratan de ser exhaustiva la lista y una lista de fuente de noticias en forma de urls list), el proceso todos los dias se ejecutara siguiendo este orden-> toma esto q te he dicho y ejecutara su modulo de extract_urls de cada fuente usando selenium y llm para discenir articulos y noticias de enlaces de otro tipo y extraera las urls y los titulos y la fecha de extraccion. Se me ha olvidado decir q cada etapa es una especioa de script standalone para tener siempre una arquitectura modula en el fliujo. Una vez extraidas las url, una etapa nueva las guarda en una tabla con columna por dato y rellenara la columna "categoria" haciendo uso del open ai api para y metiendo cada noticia por su titular en una de las categorias predefinidas. Si una noticia url ya existia se actualizara el campo de la columna last_extracted_date pero se mantendra el valor de first_extracted_date. Una vez este esto listo, se ejecutara la segunda parte del proceso que puede generar una o varias newsletter en este ultimo caso pues ejecutara el proceso tantas veces como newsletters se quieran generar. Para indicarle el numero de nuewsletters se le pasara una lista en que cada una es a su vez un set de categorias (de las que antes se definieron). La etapa siguiente sera una etapa de filtrado por categoria y por fecha (solo se quedara con las url del dia del ejecucion en first_extracted_date. Una vez las tenemos tendremos una nueva etapa llamada ranker que usando open ai se le pasara los titulares de las urls filtradas y el llm debe devolver en formato json dos listas una del total de noticia mas relevantes para hacer la newsletter del dia con el fin de estar informado de lo mas importante, pero se devolveran en json una lista de otdas las noticias "relevantes" y otra q es un subset de las relevantes de las que no solo usaremos el titular sino que el llm en etapas posteriores se leera el contenido completa del articulo. El numero de titulares y de articulos completos debe ser en base a un parametro (articulos completos siempre sera menor o igual q numero de titulares) y ademas el llm no debe incorporar varias noticias o aritculos q se refieran a lo mismo (repetidos en el fondo aunque sean de distintos medios). Las lista deberan ir ordenadas en orden de importancia descendente Una vez tenemos estas dos listas tenemos la sigueinte etapa de extract_content que obtendra el contenido de las url que queremos el contenido. Para ello tendremos a su vez aqui varias subetapas: 1) extraerlo llamando de forma clasica con python - > 2) comprobar por un llm si lo extraido es valido o si esta cortado o con error por paywall o bloqueo de algun tipo 3) si no hay paywall o bloqueo o error nos quedamos con ese contenido y con un llm parseamos la etiqueta html xpath para extraer el contenido y lo extraemos 4) si hay paywall o bloqueo pues usamos un modulo de extraccion en archive.today con proteccion antibot. 5) si este ultimo falla pues desistimos, y modificamos la lista de titulares y noticias completas pasando la noticia q dio fallo a la lista de titulares y la de mas de arriba de titulares a noticias completas -> una vez se devuelve las dos listas, una de ellas con el campo de contenido ya extraido de la misma forma que antes indique (llm parseer -> xpath -> extract) debera tmbien informar el metodo de extraccion de cada uno (libre o archiver) y si estaba inicialmente en la lista de noticias completas o en la otra lista. Una vez tenemos los contenido extraidos se pasaran los titulares y los titulares con contenido junto con un prompt de estructura y personalidad para redactar la newsletter y esta se guardara el resultado junto con su fecha y hora de generacion, fuentes consultadas y temas filtrados