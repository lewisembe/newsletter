# Stage 01: Extract URLs - DocumentaciÃ³n TÃ©cnica

> **Estado:** âœ… COMPLETADO e implementado con optimizaciones
> **Ãšltima actualizaciÃ³n:** 2025-11-10

## ğŸ“‹ Ãndice

1. [VisiÃ³n General](#visiÃ³n-general)
2. [Arquitectura](#arquitectura)
3. [Flujo de EjecuciÃ³n](#flujo-de-ejecuciÃ³n)
4. [Sistema de ClasificaciÃ³n HÃ­brido](#sistema-de-clasificaciÃ³n-hÃ­brido)
5. [DeduplicaciÃ³n y Upsert Incremental](#deduplicaciÃ³n-y-upsert-incremental)
6. [ConfiguraciÃ³n](#configuraciÃ³n)
7. [EjecuciÃ³n](#ejecuciÃ³n)
8. [Outputs y Formato](#outputs-y-formato)
9. [Optimizaciones de Costo](#optimizaciones-de-costo)
10. [Troubleshooting](#troubleshooting)
11. [MÃ©tricas y Logs](#mÃ©tricas-y-logs)

---

## ğŸ¯ VisiÃ³n General

El **Stage 01** es la primera etapa del pipeline de noticias. Su objetivo es:

1. **Extraer** todas las URLs de artÃ­culos desde fuentes de noticias configuradas usando Selenium
2. **Clasificar** URLs en 2 niveles (sintÃ¡ctico y semÃ¡ntico) usando un sistema hÃ­brido regex + LLM
3. **Deduplicar** URLs contra base de datos existente con tracking temporal dual
4. **Persistir** resultados incrementalmente en un CSV consolidado Ãºnico

### CaracterÃ­sticas Principales

- âœ… **Sistema hÃ­brido de clasificaciÃ³n:** 60-85% cobertura con regex (sin consumir tokens API)
- âœ… **ClasificaciÃ³n en 2 niveles:** SeparaciÃ³n clara entre anÃ¡lisis sintÃ¡ctico y semÃ¡ntico
- âœ… **Upsert incremental:** Guarda despuÃ©s de cada fuente (no batch al final)
- âœ… **Tracking dual de timestamps:** `extracted_at` (primera vez) + `last_extracted_at` (Ãºltima vez vista)
- âœ… **CachÃ© de URLs no-contenido:** Lookup O(1) para URLs conocidas sin contenido
- âœ… **Backups automÃ¡ticos:** Preserva versiones anteriores antes de modificar
- âœ… **Token tracking detallado:** Monitoreo de uso y costos de API
- âœ… **OptimizaciÃ³n de costos:** ~60-70% reducciÃ³n en llamadas a LLM vs enfoque naive
- âœ… **ClusterizaciÃ³n semÃ¡ntica incremental:** tras cada ingesta se ejecuta el mÃ³dulo `poc_clustering` hacia un
  Ã­ndice FAISS persistente, asignando/actualizando `cluster_id` sin borrar histÃ³ricos y almacenando embeddings para
  detecciÃ³n de duplicados entre dÃ­as (activable/desactivable con `ENABLE_NEWS_CLUSTERING`)

---

## ğŸ—ï¸ Arquitectura

### Componentes Principales

```
stages/01_extract_urls.py (script principal)
â”‚
â”œâ”€â”€ common/stage01_extraction/
â”‚   â”œâ”€â”€ selenium_utils.py          # WebDriver + scraping de URLs
â”‚   â””â”€â”€ url_classifier.py          # Clasificador hÃ­brido (regex + LLM)
â”‚
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ llm.py                     # Cliente OpenAI con token tracking
â”‚   â”œâ”€â”€ dedup.py                   # DeduplicaciÃ³n y merge de URLs
â”‚   â”œâ”€â”€ file_utils.py              # Utilidades de archivos y timestamps
â”‚   â””â”€â”€ structure_manager.py       # GestiÃ³n de directorios
â”‚
â””â”€â”€ config/
    â”œâ”€â”€ sources.yml                # ConfiguraciÃ³n de fuentes
    â”œâ”€â”€ content_categories.yml     # TaxonomÃ­a de contenido (nivel 1 y 2)
    â”œâ”€â”€ url_classification_rules.yml      # Reglas regex por fuente
    â””â”€â”€ cached_no_content_urls.yml        # CachÃ© de URLs sin contenido
```

### Tablas en `data/news.db`

- `urls`: tabla principal con cada URL extraÃ­da, su clasificaciÃ³n, timestamps y ahora la columna `cluster_id`
- `clusters`: resumen por cluster semÃ¡ntico con `id` (clave primaria), `run_date`, `article_count`, mÃ©tricas de
  similitud incremental y el `centroid_url_id` que referencia a `urls.id`. `urls.cluster_id` apunta a esta tabla, por lo
  que en ejecuciones siguientes ya sabemos cuÃ¡ntas noticias cubren ese mismo evento.
- `url_embeddings`: cachÃ© binaria de los embeddings normalizados para cada URL `contenido`. Almacenar los vectores
  permite que el Ã­ndice FAISS se reanude al iniciar Stageâ€¯01 sin re-embeder todo cada vez.

> Si tu base existe desde antes de esta versiÃ³n, ejecuta una vez
> `python scripts/migrate_add_cluster_id.py --db-path data/news.db` para aÃ±adir columnas y tablas (`clusters`,
> `url_embeddings`) junto con los Ã­ndices correspondientes.

### Clustering incremental (Stage 01.5)

- Implementado en `poc_clustering/src/persistent_clusterer.py`; usa `sentence-transformers` + `faiss-cpu`.
- El archivo `poc_clustering/config.yml` controla `similarity_threshold`, `adaptive_k`, `max_neighbors` y **el nuevo
  bloque `state.directory`**, que indica dÃ³nde persistir el Ã­ndice FAISS (`poc_clustering/state` por defecto).
- Stageâ€¯01 sÃ³lo embebe las URLs nuevas (sin `cluster_id`), las compara contra el Ã­ndice histÃ³rico y decide si van a un
  cluster existente o si crean uno nuevo. No se borra ningÃºn `cluster_id` previo.
- Las mÃ©tricas (`article_count`, similitud promedio y su media/m2 para thresholds adaptativos) se actualizan en la
  tabla `clusters`, lo que permite analizar la evoluciÃ³n de cada evento.

### Dependencias

```python
# Core
selenium>=4.0.0
openai>=1.0.0
pyyaml>=6.0
python-dotenv>=1.0.0

# Utilities
pandas>=2.0.0  # (opcional, para anÃ¡lisis)
```

### Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  sources.yml    â”‚
â”‚  (fuentes)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Selenium Driver â”‚  â† Extrae TODAS las URLs de cada fuente
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Sistema de ClasificaciÃ³n HÃ­brido            â”‚
â”‚                                                      â”‚
â”‚  1. CachÃ© URLs no-contenido (O(1) lookup)          â”‚
â”‚  2. Reglas regex por fuente (~60-85% cobertura)    â”‚
â”‚  3. Fallback a LLM (solo URLs sin coincidencia)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Nivel 1: Type  â”‚  â† SintÃ¡ctico: contenido vs no_contenido
â”‚ (regex + LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Nivel 2: Subtypeâ”‚  â† SemÃ¡ntico: noticia vs otros (OPCIONAL)
â”‚   (solo LLM)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DeduplicaciÃ³n  â”‚  â† Merge con data/raw/urls.csv existente
â”‚  Incremental    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Backup + Save   â”‚  â† Guarda despuÃ©s de cada fuente
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ data/raw/       â”‚
â”‚   urls.csv      â”‚  â† CSV consolidado Ãºnico (separador TAB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Flujo de EjecuciÃ³n

### 1. InicializaciÃ³n

```python
# 01_extract_urls.py

# Cargar configuraciÃ³n
load_dotenv()  # Variables de entorno desde .env
sources = load_yaml("config/sources.yml")
content_categories = load_yaml("config/content_categories.yml")

# Inicializar componentes
llm_client = LLMClient(api_key=os.getenv("OPENAI_API_KEY"))
classifier = URLClassifier(
    llm_client=llm_client,
    rules_path="config/url_classification_rules.yml",
    cache_path="config/cached_no_content_urls.yml"
)
driver = init_selenium_driver(headless=True)
```

### 2. ExtracciÃ³n por Fuente

```python
for source in sources['sources']:
    if not source['enabled']:
        continue

    # Navegar con Selenium
    driver.get(source['url'])
    wait_for_page_load(driver)

    # Extraer URLs usando selectores CSS con deduplicaciÃ³n intra-pÃ¡gina
    # NOTA: Si una URL aparece mÃºltiples veces, se queda con el tÃ­tulo mÃ¡s largo
    raw_urls = extract_links_with_deduplication(
        driver=driver,
        selectors=source['selectors']
    )

    # raw_urls es una lista de {'url': ..., 'title': ...}
    # Ya deduplicada a nivel de pÃ¡gina

    # Aplicar lÃ­mite de testing si estÃ¡ configurado
    if TEST_MAX_RAW_LINKS:
        raw_urls = raw_urls[:TEST_MAX_RAW_LINKS]
```

**DeduplicaciÃ³n Intra-PÃ¡gina:**

Cuando una misma URL aparece mÃºltiples veces en una pÃ¡gina (comÃºn en portales de noticias), el sistema:

1. **Detecta duplicados** por URL exacta
2. **Compara longitudes** de los tÃ­tulos extraÃ­dos
3. **Conserva el tÃ­tulo mÃ¡s largo** (asumiendo que es mÃ¡s descriptivo)
4. **Filtra tÃ­tulos cortos** (mÃ­nimo configurable con `MIN_TITLE_LENGTH`, default: 10 caracteres)

**Ejemplo:**
```
PÃ¡gina contiene:
  - <a href="/noticia-123">Ver mÃ¡s</a>           (tÃ­tulo: 8 chars)
  - <a href="/noticia-123">Crisis econÃ³mica</a>  (tÃ­tulo: 16 chars)
  - <a href="/noticia-123">Crisis econÃ³mica en Europa afecta...</a>  (tÃ­tulo: 45 chars)

Resultado:
  - URL: /noticia-123
  - TÃ­tulo: "Crisis econÃ³mica en Europa afecta..." (el mÃ¡s largo)
```

Esta lÃ³gica evita:
- TÃ­tulos genÃ©ricos como "Leer mÃ¡s", "Ver artÃ­culo"
- Duplicados innecesarios que consumirÃ­an tokens de clasificaciÃ³n
- PÃ©rdida de informaciÃ³n contextual del titular

Ver implementaciÃ³n en `common/stage01_extraction/selenium_utils.py:135-230`

### 3. ClasificaciÃ³n HÃ­brida

```python
classified_urls = []

for item in raw_urls:
    url = item['url']

    # Nivel 1: content_type (sintÃ¡ctico)
    result = classifier.classify_url_level1(
        url=url,
        source_id=source['id'],
        source_url=source['url']
    )

    # result contiene:
    # - content_type: 'contenido' | 'no_contenido'
    # - classification_method: 'cached_url' | 'regex_rule' | 'llm_api'
    # - rule_name: nombre de regla regex (o None)

    if result['content_type'] == 'no_contenido':
        continue  # Descartar

    # Nivel 2: content_subtype (semÃ¡ntico, OPCIONAL)
    if CLASSIFY_CONTENT_SUBTYPE:
        subtype = classifier.classify_url_level2(url, title)
        # subtype: 'noticia' | 'otros'
    else:
        subtype = None

    classified_urls.append({
        'url': url,
        'title': item['title'],
        'source': source['url'],
        'content_type': result['content_type'],
        'content_subtype': subtype,
        'classification_method': result['classification_method'],
        'rule_name': result['rule_name'],
        'extracted_at': datetime.now(timezone.utc).isoformat()
    })
```

### 4. DeduplicaciÃ³n Incremental

```python
# Cargar URLs existentes
existing_df = load_existing_urls("data/raw/urls.csv")

# Deduplicar y merge
merged_df = deduplicate_and_merge(
    new_urls=classified_urls,
    existing_df=existing_df,
    preserve_original_timestamp=True  # Mantener extracted_at original
)

# merged_df contiene:
# - URLs nuevas: extracted_at = last_extracted_at = NOW
# - URLs duplicadas: extracted_at = ORIGINAL, last_extracted_at = NOW
```

### 5. Persistencia Incremental

```python
# Backup automÃ¡tico antes de modificar
if urls.csv exists:
    backup_path = create_backup("data/raw/urls.csv")
    # Crea: data/raw/urls_backup_20251110_123045.csv

# Guardar CSV consolidado
save_csv(
    df=merged_df,
    path="data/raw/urls.csv",
    separator="\t",  # TAB separator
    encoding="utf-8"
)

# Actualizar reglas si estÃ¡ configurado
if UPDATE_RULES_ON_RUN:
    classifier.update_rules(
        urls=classified_urls,
        source_id=source['id']
    )
```

### 6. Logging y MÃ©tricas

```python
# Generar log estructurado
log_entry = {
    "timestamp": datetime.now(timezone.utc).isoformat(),
    "stage": "01_extract_urls",
    "source": source['id'],
    "stats": {
        "raw_urls_extracted": len(raw_urls),
        "urls_classified_contenido": len([u for u in classified_urls if u['content_type'] == 'contenido']),
        "urls_classified_no_contenido": len(raw_urls) - len(classified_urls),
        "urls_new": count_new_urls,
        "urls_duplicate": count_duplicate_urls,
        "classification_methods": {
            "cached_url": count_cached,
            "regex_rule": count_regex,
            "llm_api": count_llm
        },
        "tokens_used": llm_client.get_token_count(),
        "cost_usd": llm_client.get_cost()
    },
    "duration_seconds": elapsed_time,
    "status": "success"
}

write_log("logs/2025-11-10/01_extract_urls.log", log_entry)
```

---

## ğŸ§  Sistema de ClasificaciÃ³n HÃ­brido

### Niveles de ClasificaciÃ³n

El sistema clasifica URLs en **2 niveles independientes**:

#### Nivel 1: `content_type` (SintÃ¡ctico)
- **Objetivo:** Determinar si la URL apunta a contenido editorial o elementos auxiliares
- **Valores:** `contenido` | `no_contenido`
- **Optimizable:** âœ… SÃ­ (con reglas regex)
- **MÃ©todo:** HÃ­brido (cachÃ© â†’ regex â†’ LLM)

**Ejemplos:**
```
contenido:      https://elpais.com/internacional/2025-11-10/...
no_contenido:   https://elpais.com/suscripciones/
no_contenido:   https://elpais.com/newsletters/
no_contenido:   https://elpais.com/archivo/
```

#### Nivel 2: `content_subtype` (SemÃ¡ntico)
- **Objetivo:** Distinguir noticias de otros contenidos editoriales
- **Valores:** `noticia` | `otros` | `NULL`
- **Optimizable:** âŒ No (requiere anÃ¡lisis semÃ¡ntico)
- **MÃ©todo:** Solo LLM
- **ActivaciÃ³n:** `CLASSIFY_CONTENT_SUBTYPE=true` (default: false)

**Ejemplos:**
```
noticia:   https://bbc.com/news/world-europe-12345678
otros:     https://bbc.com/culture/article/best-films-2025
otros:     https://economist.com/the-economist-explains/...
```

### MÃ©todos de ClasificaciÃ³n

El sistema usa **4 mÃ©todos** en orden de prioridad, aplicando el primero que coincida:

| MÃ©todo | Prioridad | Consume tokens | Velocidad | Cobertura tÃ­pica | DescripciÃ³n |
|--------|-----------|----------------|-----------|------------------|-------------|
| `cached_url` | 1 | âŒ No | InstantÃ¡neo (O(1)) | 10-20% | URLs conocidas sin contenido (set lookup) |
| `heuristic` | 2 | âŒ No | Muy rÃ¡pido | 5-15% | Detecta collection pages (autor, secciÃ³n) por patrones |
| `regex_rule` | 3 | âŒ No | Muy rÃ¡pido | 40-60% | Patrones regex aprendidos por fuente |
| `llm_api` | 4 (fallback) | âœ… SÃ­ | Lento (~500ms) | 10-40% | Llamada a OpenAI GPT-4o-mini |

**Ahorro estimado:** 60-85% de URLs clasificadas SIN llamar a LLM.

### Diagrama de DecisiÃ³n de ClasificaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  URL a clasificar   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Â¿En cachÃ© no_contenido?  â”‚ â†’ SÃ â†’ âš¡ no_contenido (cached_url)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ NO
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Â¿HeurÃ­stica detecta      â”‚
    â”‚ collection page?         â”‚ â†’ SÃ â†’ âš¡ no_contenido (heuristic)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         (path corto + tÃ­tulo tipo nombre)
               â”‚ NO
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Â¿Coincide con regex      â”‚
    â”‚ rule de fuente?          â”‚ â†’ SÃ â†’ âš¡ contenido/no_contenido
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         (regex_rule: rulename)
               â”‚ NO
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Llamar a LLM             â”‚
    â”‚ (OpenAI GPT-4o-mini)     â”‚ â†’ ğŸ¤– contenido/no_contenido
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         (llm_api)
               â”‚
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Si llm_api devuelve      â”‚
    â”‚ no_contenido â†’ agregar   â”‚
    â”‚ a cachÃ© para futuro      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Notas:**
- âš¡ = Sin costo de tokens
- ğŸ¤– = Consume tokens (~$0.00003 USD por URL)
- MÃ©todos 1-3 completan en <1ms
- MÃ©todo 4 (LLM) toma ~500ms por batch de 50 URLs

### Flujo de ClasificaciÃ³n Nivel 1

```python
def classify_url_level1(url, source_id, source_url):
    """
    ClasificaciÃ³n hÃ­brida optimizada para costos.

    Orden de aplicaciÃ³n:
    1. CachÃ© URLs no-contenido (O(1) lookup)
    2. HeurÃ­sticas de collection pages (path corto + nombres)
    3. Reglas regex por fuente (~40-60% cobertura)
    4. Fallback a LLM (solo URLs sin coincidencia)
    """

    # Paso 1: Verificar cachÃ©
    if url in cached_no_content_urls:
        return {
            'content_type': 'no_contenido',
            'classification_method': 'cached_url',
            'rule_name': None
        }

    # Paso 2: Aplicar reglas regex
    rules = get_rules_for_source(source_id)
    for rule in rules:
        if re.search(rule['pattern'], url):
            return {
                'content_type': rule['content_type'],
                'classification_method': 'regex_rule',
                'rule_name': rule['name']
            }

    # Paso 3: Fallback a LLM
    prompt = f"""Analiza esta URL y clasifica...
    URL: {url}
    Fuente: {source_url}

    Responde SOLO: contenido o no_contenido"""

    response = llm_client.call(
        model=MODEL_URL_FILTER,
        prompt=prompt,
        temperature=0.0
    )

    content_type = parse_llm_response(response)

    # Actualizar cachÃ© si es no_contenido
    if content_type == 'no_contenido':
        add_to_cache(url)

    return {
        'content_type': content_type,
        'classification_method': 'llm_api',
        'rule_name': None
    }
```

### Sistema de Reglas Regex

Las reglas se almacenan en `config/url_classification_rules.yml`:

```yaml
sources:
  elpais:
    rules:
      - name: "elpais_articles"
        pattern: "^https://elpais\\.com/[^/]+/\\d{4}-\\d{2}-\\d{2}/"
        content_type: "contenido"
        coverage: 145  # URLs cubiertas en entrenamiento
        confidence: 0.95

      - name: "elpais_subscriptions"
        pattern: "/suscripciones/"
        content_type: "no_contenido"
        coverage: 23
        confidence: 1.0

      - name: "elpais_newsletters"
        pattern: "/newsletters/"
        content_type: "no_contenido"
        coverage: 12
        confidence: 1.0

  bbc:
    rules:
      - name: "bbc_news_articles"
        pattern: "^https://www\\.bbc\\.com/news/[a-z-]+-\\d+$"
        content_type: "contenido"
        coverage: 89
        confidence: 0.92
```

### GeneraciÃ³n AutomÃ¡tica de Reglas

Activar con `UPDATE_RULES_ON_RUN=true`:

```bash
UPDATE_RULES_ON_RUN=true venv/bin/python stages/01_extract_urls.py --date 2025-11-10
```

**Proceso:**
1. Clasificar TODAS las URLs con LLM (sin usar reglas)
2. Agrupar URLs por resultado (`contenido` vs `no_contenido`)
3. Analizar patrones comunes en cada grupo
4. Generar reglas regex que cumplan:
   - Cobertura mÃ­nima: `MIN_PATTERN_COVERAGE` (default: 5 URLs)
   - Porcentaje mÃ­nimo: `RULE_COVERAGE_PERCENTAGE` (default: 10%)
   - Sin falsos positivos en el training set
5. Validar reglas contra ground truth
6. Guardar en `url_classification_rules.yml`

**CuÃ¡ndo regenerar reglas:**
- Al agregar una nueva fuente
- Cambios en estructura de URLs de fuente existente
- Mensualmente (mantenimiento)
- Cuando cobertura regex cae <50%

**Costo de regeneraciÃ³n:**
- ~150-300 URLs por fuente
- ~$0.003-0.006 USD por fuente (con gpt-4o-mini)
- Recuperable en 2-3 ejecuciones normales

### CachÃ© de URLs No-Contenido

Almacenado en `config/cached_no_content_urls.yml`:

```yaml
# URLs conocidas sin contenido (lookup O(1))
cached_urls:
  - "https://elpais.com/suscripciones/"
  - "https://elpais.com/newsletters/"
  - "https://elpais.com/archivo/"
  - "https://bbc.com/newsletters"
  - "https://ft.com/myft"
  # ... (actualizado automÃ¡ticamente)

metadata:
  last_updated: "2025-11-10T10:23:45Z"
  total_urls: 347
```

**Ventajas:**
- Lookup instantÃ¡neo (O(1))
- Sin consumo de tokens
- Se actualiza automÃ¡ticamente con cada ejecuciÃ³n
- Ãštil para URLs recurrentes (footers, headers, navegaciÃ³n)

---

## ğŸ” DeduplicaciÃ³n y Upsert Incremental

### Estrategia de DeduplicaciÃ³n

```python
def deduplicate_and_merge(new_urls, existing_df, preserve_original_timestamp=True):
    """
    Merge incremental con tracking dual de timestamps.

    LÃ³gica:
    - URL nueva â†’ extracted_at = last_extracted_at = NOW
    - URL duplicada â†’ extracted_at = ORIGINAL, last_extracted_at = NOW
    """

    # Crear DataFrame de URLs nuevas
    new_df = pd.DataFrame(new_urls)

    if existing_df.empty:
        # Primera ejecuciÃ³n
        new_df['last_extracted_at'] = new_df['extracted_at']
        return new_df

    # Identificar URLs nuevas vs duplicadas
    existing_urls = set(existing_df['url'])
    new_df['is_duplicate'] = new_df['url'].isin(existing_urls)

    # Procesar duplicadas
    duplicates_mask = new_df['is_duplicate']
    for idx in new_df[duplicates_mask].index:
        url = new_df.loc[idx, 'url']
        original_row = existing_df[existing_df['url'] == url].iloc[0]

        # Preservar extracted_at original
        new_df.loc[idx, 'extracted_at'] = original_row['extracted_at']
        new_df.loc[idx, 'last_extracted_at'] = datetime.now(timezone.utc).isoformat()

    # Procesar nuevas
    new_mask = ~duplicates_mask
    new_df.loc[new_mask, 'last_extracted_at'] = new_df.loc[new_mask, 'extracted_at']

    # Merge: Remover duplicadas del DF existente, agregar todas las nuevas
    non_duplicate_existing = existing_df[~existing_df['url'].isin(new_df['url'])]
    merged_df = pd.concat([non_duplicate_existing, new_df], ignore_index=True)

    # Ordenar por last_extracted_at descendente
    merged_df = merged_df.sort_values('last_extracted_at', ascending=False)

    return merged_df
```

### Tracking Temporal Dual

Cada URL tiene **2 timestamps**:

1. **`extracted_at`** (inmutable)
   - Primera vez que la URL fue extraÃ­da
   - No cambia en re-extracciones
   - Ãštil para: filtrar noticias del dÃ­a, anÃ¡lisis histÃ³rico

2. **`last_extracted_at`** (actualizable)
   - Ãšltima vez que la URL fue vista
   - Se actualiza en cada re-extracciÃ³n
   - Ãštil para: detecciÃ³n de URLs obsoletas, freshness

**Ejemplos:**

```csv
# Primera ejecuciÃ³n (2025-11-09)
url,extracted_at,last_extracted_at
https://elpais.com/noticia-1,2025-11-09T08:00:00Z,2025-11-09T08:00:00Z

# Segunda ejecuciÃ³n (2025-11-10) - URL reaparece
url,extracted_at,last_extracted_at
https://elpais.com/noticia-1,2025-11-09T08:00:00Z,2025-11-10T08:15:00Z
                             â†‘ ORIGINAL              â†‘ ACTUALIZADO

# Segunda ejecuciÃ³n (2025-11-10) - URL nueva
url,extracted_at,last_extracted_at
https://elpais.com/noticia-2,2025-11-10T08:15:00Z,2025-11-10T08:15:00Z
                             â†‘ MISMO                â†‘ MISMO
```

### Backups AutomÃ¡ticos

Antes de modificar `data/raw/urls.csv`, se crea backup automÃ¡tico:

```python
def create_backup(csv_path):
    """
    Crea backup con timestamp antes de modificar CSV.

    Formato: urls_backup_YYYYMMDD_HHMMSS.csv
    """
    if not os.path.exists(csv_path):
        return None

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = csv_path.replace(".csv", f"_backup_{timestamp}.csv")

    shutil.copy2(csv_path, backup_path)
    logger.info(f"Backup creado: {backup_path}")

    return backup_path
```

**GestiÃ³n de backups:**
```bash
# Listar backups
ls -lh data/raw/urls_backup_*.csv

# Restaurar backup
cp data/raw/urls_backup_20251110_080000.csv data/raw/urls.csv

# Limpiar backups antiguos (manual)
find data/raw -name "urls_backup_*.csv" -mtime +30 -delete
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno (`.env`)

```env
# === OpenAI API ===
OPENAI_API_KEY=sk-xxxx

# === Modelos LLM ===
MODEL_URL_FILTER=gpt-4o-mini          # ClasificaciÃ³n nivel 1
MODEL_URL_SUBTYPE=gpt-4o-mini         # ClasificaciÃ³n nivel 2 (opcional)

# === Selenium ===
SELENIUM_HEADLESS=true
SELENIUM_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64)
SELENIUM_TIMEOUT=10                   # Timeout en segundos para carga de pÃ¡ginas y elementos

# === Stage 01: Extract URLs ===
MAX_LINKS_PER_SOURCE=250              # LÃ­mite de URLs por fuente (despuÃ©s de clasificar)
TEST_MAX_RAW_LINKS=                   # LÃ­mite para testing (vacÃ­o = sin lÃ­mite)
MIN_TITLE_LENGTH=10                   # MÃ­nimo caracteres para titular vÃ¡lido
# Nota: LÃ­mite hardcoded de 10000 enlaces raw antes de clasificaciÃ³n (no configurable)
ENABLE_NEWS_CLUSTERING=true           # Ejecuta clustering semÃ¡ntico tras cada ingesta
CLUSTERING_CONFIG_PATH=poc_clustering/config.yml  # Config de modelo/thresholds

# === URL Classification Rules ===
USE_CACHED_RULES=true                 # Usar reglas regex antes de LLM (recomendado)
UPDATE_RULES_ON_RUN=false             # Regenerar reglas automÃ¡ticamente (usa mÃ¡s tokens)
MIN_PATTERN_COVERAGE=5                # MÃ­nimo URLs que debe cubrir un patrÃ³n
RULE_COVERAGE_PERCENTAGE=10.0         # Porcentaje mÃ­nimo de cobertura

# === Content Classification Levels ===
CLASSIFY_CONTENT_SUBTYPE=false        # Nivel 2 (noticia vs otros) - false=mÃ¡s rÃ¡pido
```

### ConfiguraciÃ³n de Clustering SemÃ¡ntico

El clustering semÃ¡ntico es **opcional** y se activa con `ENABLE_NEWS_CLUSTERING=true`.

**Variables de entorno:**
```env
ENABLE_NEWS_CLUSTERING=true                      # Activar clustering incremental
CLUSTERING_CONFIG_PATH=poc_clustering/config.yml # Path al config YAML
```

**ConfiguraciÃ³n del modelo (`poc_clustering/config.yml`):**

```yaml
model:
  # Modelo de embeddings (HuggingFace)
  name: intfloat/multilingual-e5-small
  cache_dir: ./models_cache
  batch_size: 100
  device: cpu  # o 'cuda' si tienes GPU

state:
  # Directorio para Ã­ndice FAISS persistente
  directory: ./state

clustering:
  # Threshold de similitud base (0.94 = muy estricto, solo noticias casi idÃ©nticas)
  # Reduce falsos positivos en patrones estructurales similares
  similarity_threshold: 0.94

  # Threshold adaptativo por cluster (Î¼ - k*Ïƒ)
  adaptive_threshold: true
  adaptive_k: 1.1

  # TamaÃ±o mÃ­nimo de cluster para reportes
  min_cluster_size: 2

  # Vecinos a revisar en bÃºsqueda FAISS
  max_neighbors: 3
```

**Â¿QuÃ© hace el clustering?**

1. **Tras Stage 01**, se ejecuta automÃ¡ticamente `PersistentClusterer`
2. **Embebe solo URLs nuevas** (sin `cluster_id`) usando `sentence-transformers`
3. **Busca en Ã­ndice FAISS** persistente para encontrar vecinos similares
4. **Asigna cluster existente** si similitud â‰¥ threshold adaptativo, o **crea cluster nuevo**
5. **Actualiza estadÃ­sticas** en tabla `clusters` (article_count, similitud promedio)
6. **Persiste embeddings** en `url_embeddings` y actualiza Ã­ndice FAISS

**Tablas DB creadas:**

- `urls.cluster_id` â†’ FK a `clusters.id`
- `urls.cluster_assigned_at` â†’ Timestamp de asignaciÃ³n
- `clusters` â†’ Metadata por cluster (centroid, count, stats)
- `url_embeddings` â†’ Cache de vectores para Ã­ndice FAISS

**MigraciÃ³n requerida** (solo una vez):

```bash
venv/bin/python scripts/migrate_add_cluster_id.py
```

**Outputs:**

```
INFO: Clustering: 45 new URLs processed (3 clusters created)
INFO: Total clusters: 127 | Index vectors: 1523
```

**Consultar clusters:**

```bash
# Ver distribuciÃ³n de clusters
sqlite3 data/news.db "
SELECT cluster_id, COUNT(*) as size
FROM urls
WHERE cluster_id IS NOT NULL
GROUP BY cluster_id
ORDER BY size DESC
LIMIT 10;
"

# URLs de un cluster especÃ­fico
sqlite3 data/news.db "
SELECT title, url, extracted_at
FROM urls
WHERE cluster_id = '20251121_a3f8b2c4'
ORDER BY extracted_at;
"
```

**Costos:**

- **Primera ejecuciÃ³n:** ~2-3 segundos para embeding (CPU)
- **Subsiguientes:** Solo embebe URLs nuevas (incremental)
- **Sin costos LLM:** Usa embeddings locales (sentence-transformers)

### ConfiguraciÃ³n de Fuentes (`config/sources.yml`)

```yaml
sources:
  - id: "elpais"
    url: "https://elpais.com/"
    selectors:
      - "article a"
      - ".headline a"
      - "h2 a"
      - "h3 a"
    enabled: true
    notes: "Portada principal de El PaÃ­s"

  - id: "bbc"
    url: "https://www.bbc.com/news"
    selectors:
      - "a.gs-c-promo-heading"
      - "h3 a"
    enabled: true

  - id: "ft"
    url: "https://www.ft.com/"
    selectors:
      - "a.js-teaser-heading-link"
      - ".o-teaser__heading a"
    enabled: false  # Temporalmente deshabilitada
    notes: "Requiere cookies acceptance"
```

**Campos:**
- `id`: Identificador Ãºnico (usado para nombrar reglas)
- `url`: URL de la pÃ¡gina a scrapear
- `selectors`: Lista de selectores CSS para extraer enlaces (se prueban todos)
- `enabled`: Si estÃ¡ activa (false = se saltea)
- `notes`: Comentarios opcionales

### CategorÃ­as de Contenido (`config/content_categories.yml`)

```yaml
# Nivel 1: content_type (sintÃ¡ctico)
level1:
  contenido:
    description: "URLs que apuntan a contenido editorial (artÃ­culos, noticias, anÃ¡lisis)"
    examples:
      - "https://elpais.com/internacional/2025-11-10/noticia"
      - "https://bbc.com/news/world-europe-12345678"

  no_contenido:
    description: "URLs auxiliares sin contenido editorial"
    subcategories:
      navegacion:
        - "Secciones"
        - "CategorÃ­as"
        - "Tags"
      utilidades:
        - "BÃºsqueda"
        - "Archivo/hemeroteca"
        - "Mi cuenta"
      marketing:
        - "Suscripciones"
        - "Newsletters signup"
        - "Publicidad"

# Nivel 2: content_subtype (semÃ¡ntico, OPCIONAL)
level2:
  noticia:
    description: "Contenido noticioso con carÃ¡cter informativo"
    characteristics:
      - "Reporta hechos actuales"
      - "Tono neutral/objetivo"
      - "Estructura piramidal invertida"

  otros:
    description: "Otros contenidos editoriales"
    subcategories:
      - "OpiniÃ³n/editorial"
      - "AnÃ¡lisis"
      - "Reportajes/features"
      - "Entrevistas"
      - "ReseÃ±as (cultura, tecnologÃ­a)"
```

---

## ğŸš€ EjecuciÃ³n

### Modo Normal (ProducciÃ³n)

```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar con fecha especÃ­fica
python stages/01_extract_urls.py --date 2025-11-10

# O usar directamente el intÃ©rprete del venv (recomendado)
venv/bin/python stages/01_extract_urls.py --date 2025-11-10

# Usar fecha actual (se toma de system time)
venv/bin/python stages/01_extract_urls.py
```

**ConfiguraciÃ³n recomendada para producciÃ³n:**
```env
USE_CACHED_RULES=true
UPDATE_RULES_ON_RUN=false
CLASSIFY_CONTENT_SUBTYPE=false
MAX_LINKS_PER_SOURCE=250
TEST_MAX_RAW_LINKS=
```

**CaracterÃ­sticas:**
- âœ… Usa reglas regex cacheadas (mÃ¡xima eficiencia)
- âœ… No regenera reglas (sin costo adicional)
- âœ… Solo clasificaciÃ³n nivel 1 (mÃ¡s rÃ¡pido)
- âœ… LÃ­mite de 250 URLs por fuente

### Modo Testing

```bash
# Procesar solo primeras 50 URLs de cada fuente
TEST_MAX_RAW_LINKS=50 venv/bin/python stages/01_extract_urls.py --date 2025-11-10

# Sin lÃ­mite pero solo una fuente (editar sources.yml)
venv/bin/python stages/01_extract_urls.py --date 2025-11-10
```

### Regenerar Reglas (Mantenimiento Mensual)

```bash
# âš ï¸ ATENCIÃ“N: Usa MUCHOS mÃ¡s tokens (clasifica TODAS las URLs con LLM)
UPDATE_RULES_ON_RUN=true venv/bin/python stages/01_extract_urls.py --date 2025-11-10

# Con lÃ­mite de testing (para probar generaciÃ³n de reglas)
UPDATE_RULES_ON_RUN=true TEST_MAX_RAW_LINKS=100 venv/bin/python stages/01_extract_urls.py --date 2025-11-10
```

**CuÃ¡ndo usar:**
- DespuÃ©s de agregar nueva fuente
- Mensualmente (mantenimiento)
- Si cobertura de reglas cae <50%
- Cambios en estructura de URLs de fuente existente

**Costo aproximado:**
- ~$0.003-0.006 USD por fuente
- Recuperable en 2-3 ejecuciones normales

### ClasificaciÃ³n Nivel 2 (SemÃ¡ntica)

```bash
# Activar clasificaciÃ³n semÃ¡ntica (noticia vs otros)
CLASSIFY_CONTENT_SUBTYPE=true venv/bin/python stages/01_extract_urls.py --date 2025-11-10
```

**CuÃ¡ndo usar:**
- Si necesitas distinguir noticias de otros contenidos editoriales
- Para anÃ¡lisis mÃ¡s granular
- Stages posteriores requieren esta informaciÃ³n

**Trade-offs:**
- âŒ MÃ¡s lento (~50% mÃ¡s tiempo)
- âŒ MÃ¡s costoso (100% de URLs de contenido pasan por LLM)
- âœ… ClasificaciÃ³n mÃ¡s precisa para stages posteriores

### EjecuciÃ³n en ProducciÃ³n (Cron)

```bash
# Crontab: ejecutar diariamente a las 8:00 AM
0 8 * * * cd /home/user/newsletter_utils && venv/bin/python stages/01_extract_urls.py >> logs/cron.log 2>&1

# Con fecha explÃ­cita
0 8 * * * cd /home/user/newsletter_utils && venv/bin/python stages/01_extract_urls.py --date $(date +\%Y-\%m-\%d) >> logs/cron.log 2>&1
```

### Argumentos CLI

```bash
python stages/01_extract_urls.py --help

usage: 01_extract_urls.py [-h] [--date DATE]

Extract URLs from news sources (Stage 01)

optional arguments:
  -h, --help            show this help message and exit
  --date DATE           Run date (YYYY-MM-DD format). Default: today
```

**Ejemplos:**
```bash
# Fecha especÃ­fica
venv/bin/python stages/01_extract_urls.py --date 2025-11-09

# Fecha actual (default)
venv/bin/python stages/01_extract_urls.py

# Combinar con variables de entorno inline
UPDATE_RULES_ON_RUN=true venv/bin/python stages/01_extract_urls.py --date 2025-11-10
```

**Nota:** Para procesar solo una fuente o forzar actualizaciÃ³n de reglas, usar variables de entorno:
```bash
# Forzar regeneraciÃ³n de reglas
UPDATE_RULES_ON_RUN=true venv/bin/python stages/01_extract_urls.py

# Procesar solo fuentes especÃ­ficas (editar config/sources.yml: enabled: false)
```

---

## ğŸ“„ Outputs y Formato

### CSV Principal: `data/raw/urls.csv`

**CaracterÃ­sticas:**
- Separador: TAB (`\t`)
- Encoding: UTF-8
- Headers: âœ… SÃ­ (primera lÃ­nea)
- Formato: CSV estÃ¡ndar compatible con pandas, Excel, Google Sheets

**Esquema:**

| Campo | Tipo | Nullable | DescripciÃ³n |
|-------|------|----------|-------------|
| `url` | TEXT | NO | URL completa del artÃ­culo (clave Ãºnica) |
| `title` | TEXT | SÃ | TÃ­tulo extraÃ­do del elemento `<a>` |
| `source` | TEXT | NO | URL de la fuente (ej: https://elpais.com) |
| `content_type` | TEXT | NO | `contenido` \| `no_contenido` |
| `content_subtype` | TEXT | SÃ | `noticia` \| `otros` \| `NULL` |
| `classification_method` | TEXT | NO | `cached_url` \| `regex_rule` \| `llm_api` |
| `rule_name` | TEXT | SÃ | Nombre de regla regex aplicada (o `NULL`) |
| `extracted_at` | TIMESTAMP | NO | Primera vez extraÃ­da (ISO 8601, UTC) |
| `last_extracted_at` | TIMESTAMP | NO | Ãšltima vez vista (ISO 8601, UTC) |

**Ejemplo:**

```tsv
url	title	source	content_type	content_subtype	classification_method	rule_name	extracted_at	last_extracted_at
https://elpais.com/internacional/2025-11-10/crisis-europa	Crisis en Europa	https://elpais.com	contenido	NULL	regex_rule	elpais_articles	2025-11-10T08:00:00Z	2025-11-10T08:00:00Z
https://bbc.com/news/world-europe-67890123	UK Election Results	https://bbc.com/news	contenido	noticia	llm_api	NULL	2025-11-10T08:05:12Z	2025-11-10T08:05:12Z
https://ft.com/content/abc123def456	Markets rally	https://ft.com	contenido	NULL	cached_url	NULL	2025-11-09T08:00:00Z	2025-11-10T08:05:45Z
```

**Notas:**
- `title` puede ser `NULL` si no se pudo extraer del HTML
- `content_subtype` es `NULL` si `CLASSIFY_CONTENT_SUBTYPE=false`
- `rule_name` es `NULL` si se usÃ³ LLM o cachÃ©
- Timestamps en formato ISO 8601 con timezone UTC (sufijo `Z`)

### Backups: `data/raw/urls_backup_*.csv`

**Formato:** IdÃ©ntico a `urls.csv`

**Naming convention:** `urls_backup_YYYYMMDD_HHMMSS.csv`

**Ejemplos:**
```
data/raw/urls_backup_20251110_080000.csv
data/raw/urls_backup_20251110_120030.csv
data/raw/urls_backup_20251109_080000.csv
```

**GestiÃ³n:**
- Se crean automÃ¡ticamente antes de cada modificaciÃ³n
- No se eliminan automÃ¡ticamente (gestiÃ³n manual)
- Ãštil para rollback en caso de error

### Logs: `logs/YYYY-MM-DD/01_extract_urls.log`

**Formato:** JSON Lines (una lÃ­nea JSON por log entry)

**Ejemplo:**

```json
{"timestamp":"2025-11-10T08:00:05Z","stage":"01_extract_urls","event":"start","config":{"use_cached_rules":true,"update_rules_on_run":false,"classify_subtype":false,"max_links_per_source":250}}
{"timestamp":"2025-11-10T08:00:10Z","stage":"01_extract_urls","source":"elpais","event":"extraction_start","url":"https://elpais.com"}
{"timestamp":"2025-11-10T08:00:45Z","stage":"01_extract_urls","source":"elpais","event":"extraction_complete","raw_urls_extracted":312}
{"timestamp":"2025-11-10T08:01:30Z","stage":"01_extract_urls","source":"elpais","event":"classification_complete","stats":{"contenido":245,"no_contenido":67,"methods":{"cached_url":89,"regex_rule":156,"llm_api":67}}}
{"timestamp":"2025-11-10T08:01:35Z","stage":"01_extract_urls","source":"elpais","event":"dedup_complete","urls_new":198,"urls_duplicate":47}
{"timestamp":"2025-11-10T08:01:37Z","stage":"01_extract_urls","source":"elpais","event":"save_complete","csv_path":"data/raw/urls.csv","total_urls":1247}
{"timestamp":"2025-11-10T08:01:37Z","stage":"01_extract_urls","source":"elpais","event":"source_complete","duration_seconds":87.3,"tokens_used":3420,"cost_usd":0.00051}
{"timestamp":"2025-11-10T08:15:22Z","stage":"01_extract_urls","event":"complete","status":"success","sources_processed":3,"total_duration_seconds":922.8,"total_tokens_used":12845,"total_cost_usd":0.00192}
```

---

## ğŸ’° Optimizaciones de Costo

### ComparaciÃ³n: Naive vs Optimizado

#### Escenario: Extraer 1000 URLs de una fuente

**Enfoque Naive (solo LLM):**
```
1000 URLs Ã— clasificaciÃ³n nivel 1 (LLM) = 1000 llamadas API
Tokens promedio por llamada: ~200 (input) + 10 (output) = 210 tokens
Total tokens: 1000 Ã— 210 = 210,000 tokens

Costo (gpt-4o-mini):
- Input: 200,000 Ã— $0.15/1M = $0.030
- Output: 10,000 Ã— $0.60/1M = $0.006
- TOTAL: $0.036 USD
```

**Enfoque Optimizado (hÃ­brido):**
```
1. CachÃ© (20% cobertura): 200 URLs Ã— 0 tokens = 0 tokens
2. Regex (65% cobertura): 650 URLs Ã— 0 tokens = 0 tokens
3. LLM fallback (15%): 150 URLs Ã— 210 tokens = 31,500 tokens

Costo (gpt-4o-mini):
- Input: 30,000 Ã— $0.15/1M = $0.0045
- Output: 1,500 Ã— $0.60/1M = $0.0009
- TOTAL: $0.0054 USD

AHORRO: $0.036 - $0.0054 = $0.0306 USD (85% reducciÃ³n)
```

#### Escenario: EjecuciÃ³n diaria con 5 fuentes

**Naive:**
- 5 fuentes Ã— 1000 URLs Ã— $0.036 = $0.18 USD/dÃ­a
- Mensual: $0.18 Ã— 30 = $5.40 USD/mes
- Anual: $5.40 Ã— 12 = $64.80 USD/aÃ±o

**Optimizado:**
- 5 fuentes Ã— 1000 URLs Ã— $0.0054 = $0.027 USD/dÃ­a
- Mensual: $0.027 Ã— 30 = $0.81 USD/mes
- Anual: $0.81 Ã— 12 = $9.72 USD/aÃ±o

**AHORRO ANUAL: $55.08 USD (85% reducciÃ³n)**

### OptimizaciÃ³n Adicional: HeurÃ­sticas de Collection Pages

AdemÃ¡s del sistema regex + cachÃ©, existe una **capa adicional de optimizaciÃ³n** que detecta pÃ¡ginas de autor/secciÃ³n SIN llamar al LLM:

**HeurÃ­sticas implementadas:**

1. **Path corto + TÃ­tulo tipo nombre**
   ```
   URL: https://elpais.com/autor/john-smith/
   TÃ­tulo: "John Smith"
   â†’ Detecta: pÃ¡gina de autor (NO artÃ­culo)
   ```

2. **Patrones comunes de collection pages**
   ```python
   patterns = [
       '/author/', '/columnist/', '/writers/',
       '/by/', '/perfil/', '/profile/',
       '/category/', '/section/', '/tema/'
   ]
   ```

3. **TÃ­tulo con nombre propio sin verbos**
   - Detecta nombres (mayÃºsculas consecutivas)
   - Verifica ausencia de verbos comunes
   - Si coincide â†’ `no_contenido (heuristic)`

**Impacto:**
- **Ahorro adicional:** 5-15% de URLs (varÃ­a por fuente)
- **Sin costo:** Cero tokens consumidos
- **Velocidad:** <1ms por URL
- **AplicaciÃ³n:** Antes de regex rules

**Ejemplo real:**
```
# Portada de BBC con 200 enlaces
- 25 URLs â†’ cached_url (10%)
- 30 URLs â†’ heuristic (15%)  â† OPTIMIZACIÃ“N ADICIONAL
- 100 URLs â†’ regex_rule (50%)
- 45 URLs â†’ llm_api (25%)

Total sin LLM: 155/200 (77.5%)
Tokens ahorrados: 155 Ã— 210 = 32,550 tokens (~$0.005 USD)
```

Ver implementaciÃ³n en `common/llm.py:378-468`

### Recomendaciones de OptimizaciÃ³n

1. **Mantener reglas actualizadas:**
   - Regenerar mensualmente con `UPDATE_RULES_ON_RUN=true`
   - Revisar fuentes con baja cobertura

2. **Desactivar nivel 2 en producciÃ³n:**
   - `CLASSIFY_CONTENT_SUBTYPE=false` (default)
   - Activar solo si stages posteriores lo requieren

3. **Ajustar lÃ­mites por fuente:**
   - `MAX_LINKS_PER_SOURCE=250` es razonable
   - Reducir si hay muchas fuentes

4. **Usar modelo mÃ¡s barato:**
   - `MODEL_URL_FILTER=gpt-4o-mini` (recomendado)
   - No usar `gpt-4o` salvo necesidad especÃ­fica

5. **Monitorear mÃ©tricas:**
   - Revisar logs de costo diariamente
   - Establecer alertas si costo >$X/dÃ­a

---

## ğŸ“Š Token Usage Tracking

Stage 01 incluye un sistema completo de tracking de consumo de tokens OpenAI, implementado en `common/token_tracker.py`.

### MÃ©tricas Rastreadas

```python
{
    "prompt_tokens": 1234,        # Tokens enviados a OpenAI
    "completion_tokens": 567,     # Tokens recibidos de OpenAI
    "total_tokens": 1801,         # Suma de ambos
    "cost_usd": 0.000123,         # Costo estimado en USD
    "model": "gpt-4o-mini",       # Modelo usado
    "task": "url_filter"          # Tarea especÃ­fica
}
```

### UbicaciÃ³n de Logs

Los tokens se registran en **dos ubicaciones**:

1. **Console output:**
   ```
   [2025-11-10 08:15:32] Completed source: elpais (120 contenido, 85 no_contenido)
   [2025-11-10 08:15:32] Tokens used: 12,450 prompt + 3,200 completion = 15,650 total (~$0.0045 USD)
   ```

2. **Archivo de log:**
   ```
   logs/2025-11-10/01_extract_urls.log
   ```

   Ejemplo de entrada:
   ```
   [2025-11-10 08:15:32] INFO - Token usage for source 'elpais':
     - Prompt tokens: 12,450
     - Completion tokens: 3,200
     - Total tokens: 15,650
     - Estimated cost: $0.0045 USD
     - Model: gpt-4o-mini
   ```

### CÃ¡lculo de Costos

**Precios actuales (Enero 2025):**
```python
# gpt-4o-mini
prompt_cost = $0.150 / 1M tokens
completion_cost = $0.600 / 1M tokens

# gpt-4o (NO recomendado para Stage 01)
prompt_cost = $2.50 / 1M tokens
completion_cost = $10.00 / 1M tokens
```

**FÃ³rmula:**
```python
cost_usd = (prompt_tokens / 1_000_000) * prompt_price + \
           (completion_tokens / 1_000_000) * completion_price
```

### Consumo TÃ­pico por Fuente

**Sin optimizaciones (100% LLM):**
```
Fuente: BBC News (200 URLs)
- Prompt tokens: ~42,000 (210 tokens/URL Ã— 200)
- Completion tokens: ~10,000 (50 tokens/URL Ã— 200)
- Total: 52,000 tokens
- Costo: ~$0.012 USD
```

**Con optimizaciones (25% LLM):**
```
Fuente: BBC News (200 URLs)
- URLs clasificadas sin LLM: 150 (75%)
  - Cached URLs: 20 (10%)
  - Heuristics: 30 (15%)
  - Regex rules: 100 (50%)
- URLs clasificadas con LLM: 50 (25%)
  - Prompt tokens: ~10,500
  - Completion tokens: ~2,500
  - Total: 13,000 tokens
  - Costo: ~$0.003 USD

Ahorro: $0.009 USD (75% reducciÃ³n)
```

### Tracking en el CÃ³digo

**InicializaciÃ³n:**
```python
from common.token_tracker import TokenTracker

tracker = TokenTracker()
```

**Registrar uso:**
```python
# DespuÃ©s de cada llamada a OpenAI
response = client.chat.completions.create(...)

tracker.track(
    prompt_tokens=response.usage.prompt_tokens,
    completion_tokens=response.usage.completion_tokens,
    model="gpt-4o-mini",
    task="url_classification_level1"
)
```

**Obtener resumen:**
```python
summary = tracker.get_summary()
print(f"Total tokens: {summary['total_tokens']:,}")
print(f"Total cost: ${summary['cost_usd']:.4f} USD")
```

### Alertas de Costo

Para establecer alertas automÃ¡ticas, agregar al `.env`:

```env
# Alertas de costo (futuro - no implementado aÃºn)
MAX_COST_PER_RUN_USD=0.50          # Cancelar si costo > $0.50
WARN_COST_PER_SOURCE_USD=0.05      # Warning si fuente > $0.05
EMAIL_ALERTS=true
ALERT_EMAIL=admin@example.com
```

**Nota:** Las alertas estÃ¡n diseÃ±adas pero **NO implementadas** en Stage 01 actual.

### AnÃ¡lisis de Eficiencia

Para analizar la eficiencia del sistema de clasificaciÃ³n:

```bash
# Ver logs de token usage
cat logs/2025-11-10/01_extract_urls.log | grep "Token usage"

# Sumar tokens totales del dÃ­a
cat logs/2025-11-10/01_extract_urls.log | grep "total_tokens" | awk '{sum+=$NF} END {print sum}'

# Calcular costo total del dÃ­a
cat logs/2025-11-10/01_extract_urls.log | grep "cost_usd" | awk '{sum+=$NF} END {printf "%.4f\n", sum}'
```

### MÃ©tricas de ClasificaciÃ³n

AdemÃ¡s del tracking de tokens, Stage 01 registra mÃ©tricas de clasificaciÃ³n:

```
[2025-11-10 08:15:32] Classification breakdown for source 'elpais':
  - cached_url: 20 URLs (10.0%)
  - heuristic: 30 URLs (15.0%)
  - regex_rule: 100 URLs (50.0%)
  - llm_api: 50 URLs (25.0%)

  Total without LLM: 150/200 (75.0%)
  Tokens saved: ~39,000 tokens (~$0.009 USD)
```

Estas mÃ©tricas permiten:
- Identificar fuentes con baja cobertura de reglas
- Calcular ROI del sistema de optimizaciÃ³n
- Detectar oportunidades de mejora

Ver `common/llm.py:800-850` para implementaciÃ³n completa.

---

## ğŸ› Troubleshooting

### Errores Comunes

#### 1. Selenium WebDriver no inicia

**SÃ­ntomas:**
```
selenium.common.exceptions.WebDriverException: Message: 'chromedriver' executable needs to be in PATH
```

**SoluciÃ³n:**
```bash
# Verificar que Chrome/Chromium estÃ¡ instalado
google-chrome --version

# Instalar chromedriver manualmente
sudo apt-get install chromium-chromedriver  # Linux
brew install chromedriver                  # macOS

# O usar webdriver-manager (instalado con requirements.txt)
python -c "from webdriver_manager.chrome import ChromeDriverManager; ChromeDriverManager().install()"
```

#### 2. OpenAI API Key invÃ¡lida

**SÃ­ntomas:**
```
openai.error.AuthenticationError: Incorrect API key provided
```

**SoluciÃ³n:**
```bash
# Verificar que .env contiene la key correcta
cat .env | grep OPENAI_API_KEY

# Probar key manualmente
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

#### 3. CSV corrupto o mal formateado

**SÃ­ntomas:**
```
pandas.errors.ParserError: Error tokenizing data
```

**SoluciÃ³n:**
```bash
# Verificar encoding
file data/raw/urls.csv

# Contar columnas
head -1 data/raw/urls.csv | awk -F'\t' '{print NF}'

# Restaurar desde backup
cp data/raw/urls_backup_20251110_080000.csv data/raw/urls.csv
```

#### 4. Timeout en extracciÃ³n de URLs

**SÃ­ntomas:**
```
selenium.common.exceptions.TimeoutException: Message:
```

**SoluciÃ³n:**
```python
# Aumentar timeouts en selenium_utils.py
WAIT_TIMEOUT = 30  # Default: 10
PAGE_LOAD_TIMEOUT = 60  # Default: 30
```

#### 5. URLs duplicadas con tÃ­tulos diferentes

**SÃ­ntomas:**
```
# Misma URL aparece mÃºltiples veces en logs
Skipped link with short title (8 chars): /noticia-123 â†’ 'Ver mÃ¡s'
Replacing link for /noticia-123: new title longer (45 vs 16 chars)
```

**ExplicaciÃ³n:**
Esto es **comportamiento esperado**. El sistema deduplica URLs dentro de una misma pÃ¡gina y conserva el tÃ­tulo mÃ¡s largo. Ver secciÃ³n "DeduplicaciÃ³n Intra-PÃ¡gina" en stages/README_STAGE01.md:183.

**Ajustes:**
```env
# Cambiar longitud mÃ­nima de tÃ­tulo (default: 10)
MIN_TITLE_LENGTH=15  # MÃ¡s estricto
MIN_TITLE_LENGTH=5   # MÃ¡s permisivo
```

#### 6. Muchos enlaces descartados por tÃ­tulo corto

**SÃ­ntomas:**
```
Skipped link with short title (5 chars): https://... â†’ 'MÃ¡s'
Skipped link with short title (7 chars): https://... â†’ 'Leer ++'
```

**SoluciÃ³n:**
```bash
# Reducir el mÃ­nimo de caracteres requerido
MIN_TITLE_LENGTH=5 venv/bin/python stages/01_extract_urls.py

# O revisar selectores CSS en sources.yml para ser mÃ¡s especÃ­fico
```

---

## âš ï¸ Limitaciones y Restricciones Conocidas

### 1. **Manejo de Cookies NO Implementado**

**DescripciÃ³n:**
El sistema NO maneja automÃ¡ticamente banners de cookies o aceptaciÃ³n de tÃ©rminos.

**Fuentes afectadas:**
- Financial Times (requiere aceptar cookies)
- Algunos sitios de BBC (ocasionalmente)
- Sitios con GDPR compliance obligatorio

**SoluciÃ³n actual:**
```yaml
# config/sources.yml
- id: "ft"
  url: "https://www.ft.com/"
  enabled: false  # Deshabilitada por cookies
  notes: "Requiere cookies acceptance"
```

**Workaround manual:**
1. Abrir el sitio manualmente en navegador
2. Aceptar cookies
3. Copiar cookies del navegador al script (NO implementado)
4. O implementar lÃ³gica custom de click en banner (futuro)

### 2. **LÃ­mite Hardcoded de 10,000 Enlaces**

**DescripciÃ³n:**
Antes de clasificar, hay un lÃ­mite hardcoded de 10,000 enlaces raw por fuente.

**UbicaciÃ³n:** `stages/01_extract_urls.py:156` â†’ `max_links=10000`

**NO configurable** vÃ­a `.env` o argumentos CLI.

**Impacto:**
Si una fuente tiene >10,000 enlaces (raro), se procesarÃ¡n solo los primeros 10,000.

**RazÃ³n del lÃ­mite:**
- ProtecciÃ³n contra scraping infinito
- PrevenciÃ³n de out-of-memory
- Control de costos de API

**ConfiguraciÃ³n recomendada:**
```env
# LÃ­mites efectivos por fuente (en orden de aplicaciÃ³n):
# 1. Hardcoded: 10000 enlaces raw (antes de clasificaciÃ³n)
# 2. TEST_MAX_RAW_LINKS: opcional, para testing (antes de clasificaciÃ³n)
# 3. MAX_LINKS_PER_SOURCE: 250 (despuÃ©s de clasificaciÃ³n)
```

### 3. **Batch Size de LLM NO Configurable**

**DescripciÃ³n:**
El sistema clasifica URLs en batches de 50 (hardcoded).

**UbicaciÃ³n:** `common/llm.py:116`, `llm.py:478`, `llm.py:752`

**Problema:**
Usuarios con rate limits estrictos de OpenAI no pueden reducir el batch size sin editar cÃ³digo.

**Workaround:**
```bash
# Reducir URLs totales procesadas
TEST_MAX_RAW_LINKS=25 venv/bin/python stages/01_extract_urls.py
```

### 4. **Precios de Tokens Hardcoded**

**DescripciÃ³n:**
Los costos estimados usan precios de OpenAI hardcoded en `token_tracker.py:103-120`.

**ActualizaciÃ³n:** Precios de 2025-01

**Riesgo:**
Si OpenAI cambia precios, los costos reportados estarÃ¡n desactualizados.

**RecomendaciÃ³n:**
Verificar costos reales en dashboard de OpenAI mensualmente.

### 5. **Separador TAB Obligatorio**

**DescripciÃ³n:**
Los CSV usan separador TAB (`\t`) hardcoded, NO configurable.

**RazÃ³n:**
TÃ­tulos de noticias suelen contener comas, lo que romperÃ­a CSV con separador `,`.

**ImplicaciÃ³n:**
- Compatible con Excel, Google Sheets, pandas
- NO compatible con herramientas que esperan comas
- ConversiÃ³n manual requerida si se necesita formato diferente

### 6. **Condiciones de Parada del Scrolling**

**DescripciÃ³n:**
El scroll automÃ¡tico se detiene en 2 condiciones:

1. **LÃ­mite alcanzado:** 10,000 enlaces (o `TEST_MAX_RAW_LINKS`)
2. **Sin nuevos enlaces:** 3 iteraciones consecutivas sin encontrar URLs nuevas

**ImplicaciÃ³n:**
Sitios con lazy-loading lento pueden detener el scroll prematuramente.

**Ajuste:**
```env
# Aumentar timeout para sitios lentos
SELENIUM_TIMEOUT=30  # Default: 10
```

### 7. **Sin Soporte para JavaScript-heavy Sites**

**DescripciÃ³n:**
Sitios que cargan TODO el contenido vÃ­a JavaScript asÃ­ncrono pueden no funcionar.

**Ejemplos problemÃ¡ticos:**
- SPAs (Single Page Applications) con routing client-side
- Infinite scroll sin URLs estÃ¡ticas
- Contenido protegido por anti-scraping

**DetecciÃ³n:**
```bash
# Si logs muestran 0 enlaces extraÃ­dos consistentemente
Found 0 elements with selector: article a
```

**SoluciÃ³n:**
Revisar selectores CSS en `sources.yml` o descartar la fuente.

---

## ğŸ“š Referencias

### DocumentaciÃ³n Relacionada

- **[CLAUDE.md](../CLAUDE.md)** - DocumentaciÃ³n completa del proyecto
- **[URL_CLASSIFICATION_RULES.md](../URL_CLASSIFICATION_RULES.md)** - Detalles del sistema de reglas
- **[README.md](../README.md)** - GuÃ­a de inicio rÃ¡pido

### Archivos Clave

- `stages/01_extract_urls.py` - Script principal (stages/01_extract_urls.py:1)
- `common/stage01_extraction/selenium_utils.py` - Utilidades Selenium
- `common/stage01_extraction/url_classifier.py` - Clasificador hÃ­brido
- `common/llm.py` - Cliente LLM (common/llm.py:1)
- `common/dedup.py` - DeduplicaciÃ³n (common/dedup.py:1)
- `config/sources.yml` - ConfiguraciÃ³n de fuentes
- `config/url_classification_rules.yml` - Reglas regex
- `config/cached_no_content_urls.yml` - CachÃ© de URLs

---

**Ãšltima actualizaciÃ³n:** 2025-11-10
**VersiÃ³n:** 1.0.0
**Estado:** ProducciÃ³n âœ…
