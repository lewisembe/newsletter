# Stage 04: Extract Content

**Estado:** ‚úÖ Implementado con Smart Substitution | **√öltima actualizaci√≥n:** 2025-11-16

## üéØ Objetivo

Extraer contenido completo de art√≠culos rankeados usando m√∫ltiples m√©todos de extracci√≥n con fallback inteligente, detecci√≥n de paywalls y **sustituci√≥n autom√°tica** para garantizar N art√≠culos con contenido.

## üÜï Nueva Funcionalidad: Smart Substitution

**Problema anterior:** Si un art√≠culo featured fallaba extracci√≥n ‚Üí se quedaba sin contenido
**Soluci√≥n nueva:** Sustituir autom√°ticamente usando pool de art√≠culos no-featured

### Prioridad de Sustituci√≥n (Dual Subset Format)

El sistema detecta autom√°ticamente el formato del JSON de entrada y aplica la estrategia correcta:

#### **Para Dual Subset (formato actual - post clustering removal):**

1. **Featured article falla** ‚Üí Intentar con art√≠culos non-featured (en orden de rank)
2. **Mantener headlines originales** ‚Üí El ranking visible (top 20) no cambia
3. **Sustituir solo contenido** ‚Üí Los titulares se mantienen, pero el contenido se extrae del pool de backups
4. **Repetir hasta conseguir N art√≠culos** o agotar candidatos

**Ventajas:**
- ‚úÖ Los non-featured (ranks 11-20) sirven como pool de sustituci√≥n
- ‚úÖ No requiere clustering
- ‚úÖ Headlines siempre consistentes (top 20)
- ‚úÖ Tracking completo de sustituciones

#### **Para Legacy/Clustering Format (retrocompatibilidad):**

1. **Art√≠culo primary falla** ‚Üí Intentar con sus `related_articles` espec√≠ficos (mismo cluster)
2. **Todos los related fallan** ‚Üí Sustituir con siguiente art√≠culo primary de la lista
3. **Repetir hasta conseguir N art√≠culos** o agotar candidatos

### Ejemplo de Ejecuci√≥n (Dual Subset)

```
Target: 10 art√≠culos con contenido
Input: 19 URLs (10 featured, 9 non-featured)

Candidate pool: [Featured #1-10, Non-Featured #11-19] (19 candidatos totales)

Featured #1 (rank 1) ‚Üí ‚úì √âxito
Featured #2 (rank 2) ‚Üí ‚úó Fallo (paywall NYT)
Featured #3 (rank 3) ‚Üí ‚úó Fallo (paywall)
Featured #4 (rank 4) ‚Üí ‚úì √âxito
Featured #5-10 ‚Üí ‚úì Todos exitosos (6 art√≠culos)

‚úì 8 featured exitosos, faltan 2 para llegar a 10
‚Üí Continuar con pool de sustituci√≥n (non-featured):

Non-Featured #11 ‚Üí ‚úì √âxito (SUSTITUCI√ìN para slot #2)
Non-Featured #12 ‚Üí ‚úì √âxito (SUSTITUCI√ìN para slot #3)

Resultado: 10 art√≠culos con contenido (2 sustituciones)
Headlines mostradas: Ranks 1-20 (sin cambios)
Contenido extra√≠do: Featured #1,4-10 + Non-Featured #11,12
```

### Metadata de Sustituci√≥n

Cuando ocurre una sustituci√≥n, se a√±ade metadata completa:

```json
{
  "rank": 2,
  "id": 4002,
  "title": "Non-featured article title",
  "was_substitution": true,
  "substitution_source_rank": 11,
  "original_target_rank": 2,
  "substituted_for_id": 3638,
  "candidate_type": "substitute"
}
```

**Campos:**
- `was_substitution`: true si el contenido proviene de un art√≠culo diferente al headline
- `substitution_source_rank`: Rank original del art√≠culo que proporcion√≥ el contenido (ej: 11)
- `original_target_rank`: Rank del slot que necesitaba contenido (ej: 2)
- `substituted_for_id`: ID del art√≠culo featured que fall√≥ extracci√≥n
- `candidate_type`: Tipo de candidato (`featured`, `substitute`, `primary`, `related`)

## üèóÔ∏è Arquitectura

```
Input: ranked_YYYY-MM-DD_HHMMSS_*.json (del Stage 03)
  ‚Üì
1. Construir candidate pool con prioridad:
   - Primary #1, Related #1.1, Related #1.2, ...
   - Primary #2, Related #2.1, Related #2.2, ...
   - Primary #3, ...
  ‚Üì
2. Para cada candidato (hasta conseguir N con contenido):
   a. Fetch directo
   b. Detecci√≥n de paywall (LLM)
   c. Archive.today si paywall detectado
   d. Extracci√≥n cascada:
      - XPath Cache (si existe) ‚Üí GRATIS
      - newspaper3k ‚Üí GRATIS
      - readability ‚Üí GRATIS
      - LLM XPath Discovery ‚Üí PAGO (~$0.0003)
   e. Limpieza de contenido
   f. Guardar en BD
   g. Si √©xito ‚Üí Siguiente target rank
   h. Si fallo ‚Üí Siguiente candidato (sustituci√≥n)
  ‚Üì
Output:
  - BD actualizada con full_content
  - JSON con execution_report detallado
```

## üöÄ Uso

### B√°sico (con sustituci√≥n autom√°tica)
```bash
# Extraer de un ranking espec√≠fico
# Por defecto: intenta conseguir contenido para TODOS los art√≠culos primarios
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_2025-11-13_143052_level_top25_all_cluster2x.json
```

### Target N art√≠culos con contenido
```bash
# Intentar conseguir exactamente 10 art√≠culos con contenido
# Si alguno falla ‚Üí sustituye autom√°ticamente con related/siguientes
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --max-articles 10
```

### Deshabilitar sustituci√≥n
```bash
# Solo intentar art√≠culos primarios (sin sustituciones)
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --disable-substitution
```

### Opciones Avanzadas
```bash
# Re-extraer aunque ya exista contenido
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --force

# Skip paywall check (√∫til para testing)
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --skip-paywall-check

# Modo verbose
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --verbose

# Combinaciones
venv/bin/python stages/04_extract_content.py \
    --input data/processed/ranked_X.json \
    --max-articles 15 \
    --force \
    --verbose
```

## üìä M√©todos de Extracci√≥n

### 1. XPath Cache (Prioridad 1)
- **C√≥mo funciona:** Reutiliza selectores CSS/XPath descubiertos previamente
- **Cache:** `config/xpath_cache.yml`
- **Ventaja:** Instant√°neo, gratis, ~95% √©xito despu√©s de poblado
- **Costo:** $0

**Ejemplo de cache:**
```yaml
www.elconfidencial.com:
  content_selector: "article.news-body p"
  selector_type: "css"
  confidence: 95
  success_rate: 0.96
```

### 2. newspaper3k (Fallback 1)
- **C√≥mo funciona:** Heur√≠sticas autom√°ticas para detectar contenido
- **Detecta:** `<article>`, `<main>`, `.post-content`, densidad de texto
- **√âxito:** ~70-80%
- **Costo:** $0

### 3. readability-lxml (Fallback 2)
- **C√≥mo funciona:** Algoritmo de Mozilla (Firefox Reader Mode)
- **Calcula:** Scores por nodo basado en texto/HTML ratio
- **√âxito:** ~60-70%
- **Costo:** $0

### 4. LLM XPath Discovery (Fallback 3)
- **C√≥mo funciona:** LLM analiza HTML y descubre selector √≥ptimo
- **Prompt:** Primeros 8000 chars del HTML
- **Ventaja:** Inteligente, adapta a cualquier estructura
- **Cache:** Guarda selector descubierto para pr√≥ximas veces
- **Costo:** ~$0.0003 por art√≠culo (~600 tokens)

## üõ°Ô∏è Detecci√≥n de Paywall

### Estrategia: LLM con optimizaci√≥n de tokens
```python
# En lugar de pasar HTML completo (8000 chars = ~2000 tokens):
inicio = html[:500]    # Primeros 500 chars
final = html[-1000:]   # √öltimos 1000 chars

# Pasar solo inicio + final (~600 tokens)
# Ahorro: 70% tokens
```

**Se√±ales de paywall:**
- "Suscr√≠bete para continuar"
- "Este contenido es exclusivo"
- "Register to read"
- Contenido cortado abruptamente

**Costo:** ~$0.00009 por validaci√≥n (600 tokens in + 5 out)

### Fallback: archive.today
Si paywall detectado:
1. Fetch con Selenium (anti-bot measures)
2. Wait 8 segundos (JavaScript execution)
3. Detectar CAPTCHA (<5000 bytes)
4. Retry con backoff (max 2 intentos)

**√âxito:** ~90% con Selenium

## üîß Configuraci√≥n (.env)

```env
# Stage 04: Extract Content
STAGE04_TIMEOUT=30                      # HTTP timeout (segundos)
STAGE04_ARCHIVE_WAIT_TIME=15            # Selenium wait (segundos)
STAGE04_MAX_RETRIES=2                   # Reintentos para archive
STAGE04_MIN_WORD_COUNT=100              # M√≠nimo palabras v√°lidas
STAGE04_MAX_WORD_COUNT=10000            # M√°ximo (trunca)

# Models
MODEL_PAYWALL_VALIDATOR=gpt-4o-mini    # Detecci√≥n paywall
MODEL_XPATH_DISCOVERY=gpt-4o-mini      # Descubrimiento XPath

# Cache
XPATH_CACHE_PATH=config/xpath_cache.yml
```

## üìÅ Estructura de Archivos

```
common/stage04_extraction/
‚îú‚îÄ‚îÄ __init__.py              # Exports p√∫blicos
‚îú‚îÄ‚îÄ xpath_cache.py           # Gesti√≥n de cache de selectores
‚îú‚îÄ‚îÄ paywall_validator.py     # Detecci√≥n de paywalls con LLM
‚îú‚îÄ‚îÄ archive_fetcher.py       # Fetch desde archive.today
‚îú‚îÄ‚îÄ extractors.py            # M√©todos de extracci√≥n (newspaper, readability, LLM)
‚îî‚îÄ‚îÄ content_cleaner.py       # Limpieza de boilerplate
```

## üí∞ Costos

**Escenario: 25 art√≠culos**

| Operaci√≥n | Frecuencia | Costo Unitario | Total |
|-----------|------------|----------------|-------|
| Paywall validation (LLM) | 25x (siempre) | $0.00009 | $0.00225 |
| XPath Cache | 10x (40%) | $0 | $0 |
| newspaper3k | 8x (30%) | $0 | $0 |
| readability | 2x (10%) | $0 | $0 |
| LLM XPath Discovery | 5x (20%) | $0.0003 | $0.0015 |
| **TOTAL** | | | **~$0.004** |

**Despu√©s de 10 ejecuciones:** Cache poblado ‚Üí $0.0025 por ejecuci√≥n

## üóÑÔ∏è Base de Datos

**Columnas actualizadas:**
- `full_content` - Contenido extra√≠do limpio
- `extraction_status` - `success`/`failed`/`pending`
- `extraction_error` - Mensaje de error
- `word_count` - Palabras extra√≠das
- `content_extraction_method` - M√©todo usado
- `content_extracted_at` - Timestamp
- `archive_url` - URL de archive si se us√≥

## üìä Salida

### 1. Logs
```
logs/YYYY-MM-DD/04_extract_content.log
```

### 2. JSON con Execution Report
```json
{
  "run_date": "2025-11-13",
  "generated_at": "2025-11-13T14:30:52Z",
  "input_file": "data/processed/ranked_2025-11-13_143052.json",
  "execution_report": {
    "target_articles_count": 25,
    "final_articles_with_content": 23,
    "total_attempts": 28,
    "successful_extractions": 23,
    "failed_extractions": 5,
    "substitutions_made": 3,
    "status": "partial_success_with_substitutions",
    "details": [
      {
        "target_rank": 1,
        "url_id": 123,
        "title": "Breaking news...",
        "candidate_type": "primary",
        "extraction_status": "success",
        "word_count": 847,
        "method": "xpath_cache"
      },
      {
        "target_rank": 5,
        "url_id": 456,
        "title": "Primary article...",
        "candidate_type": "primary",
        "extraction_status": "failed",
        "error": "Paywall cannot be bypassed"
      },
      {
        "target_rank": 5,
        "url_id": 457,
        "title": "Related article...",
        "candidate_type": "related",
        "parent_id": 456,
        "extraction_status": "success",
        "word_count": 623,
        "method": "newspaper",
        "substitution_reason": "Primary article extraction failed"
      }
    ]
  },
  "articles": [
    {
      "rank": 1,
      "id": 123,
      "url": "https://...",
      "title": "...",
      "source": "ft.com",
      "categoria_tematica": "economia",
      "word_count": 847,
      "extraction_method": "xpath_cache",
      "was_substitution": false,
      "original_rank": 1
    }
  ]
}
```

**Archivo:** `data/processed/content_YYYY-MM-DD_HHMMSS.json`

### 3. Resumen en logs
```
STAGE 04 SUMMARY
================================================================================
Total URLs processed: 28
  ‚úì Successfully extracted: 23
  ‚Üí Skipped (already extracted): 0
  ‚úó Failed: 5
  üì¶ Used archive.today: 5

Extraction methods used:
  xpath_cache: 10
  newspaper: 8
  readability: 3
  llm_xpath: 2

Average word count: 847 words

SUBSTITUTION REPORT:
  Target articles: 25
  Final with content: 23
  Substitutions made: 3
  Status: partial_success_with_substitutions
================================================================================
```

### Interpretaci√≥n de Status

| Status | Significado |
|--------|-------------|
| `success` | Alcanz√≥ target sin sustituciones |
| `success_with_substitutions` | Alcanz√≥ target con sustituciones |
| `partial_success` | No alcanz√≥ target pero tiene art√≠culos |
| `partial_success_with_substitutions` | No alcanz√≥ target, us√≥ sustituciones |
| `failed` | No consigui√≥ ning√∫n art√≠culo |

## üêõ Troubleshooting

### Error: "Direct fetch failed"
```bash
# Verificar conectividad
curl -I https://www.example.com

# Revisar timeout
STAGE04_TIMEOUT=60 venv/bin/python stages/04_extract_content.py ...
```

### Error: "All extraction methods failed"
```bash
# Probar con LLM m√°s potente
MODEL_XPATH_DISCOVERY=gpt-4o venv/bin/python stages/04_extract_content.py ...

# Ver logs detallados
venv/bin/python stages/04_extract_content.py ... --verbose
```

### XPath Cache con baja tasa de √©xito
```bash
# Limpiar entradas malas
venv/bin/python -c "
from common.stage04_extraction import cleanup_xpath_cache
cleanup_xpath_cache(min_success_rate=0.7)
"
```

### Archive.today bloqueado (CAPTCHA)
- Aumentar `STAGE04_ARCHIVE_WAIT_TIME=20`
- Reducir frecuencia de requests
- Considerar usar proxy/VPN

## üîÑ Pr√≥ximos Pasos

- [ ] **Testing:** Unit tests para extractors
- [ ] **Optimizaci√≥n:** Parallel fetching con asyncio
- [ ] **M√©tricas:** Dashboard de success rates por dominio
- [ ] **Stage 05:** Usar `full_content` para generar newsletters

---

**Ver tambi√©n:**
- `config/xpath_cache.yml` - Cache de selectores por dominio
- `common/stage04_extraction/` - C√≥digo fuente de utilities
- `CLAUDE.md` - Overview completo del proyecto
